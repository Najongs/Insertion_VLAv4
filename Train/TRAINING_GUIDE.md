# SmolVLA Training Guide - New Dataset

ìƒˆë¡œìš´ HDF5 ë°ì´í„°ì…‹ìœ¼ë¡œ SmolVLA ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
Train/
â”œâ”€â”€ train_smolvla_new_dataset.py      # í•™ìŠµ ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ hdf5_lerobot_adapter.py           # HDF5 ë°ì´í„° ì–´ëŒ‘í„°
â”œâ”€â”€ train_config_new_dataset.yaml     # í•™ìŠµ ì„¤ì • íŒŒì¼
â”œâ”€â”€ train_single_gpu.sh               # ë‹¨ì¼ GPU ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ train_multi_gpu.sh                # ë©€í‹° GPU ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ train_background.sh               # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ stop_training.sh                  # í•™ìŠµ ì¤‘ë‹¨ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ TRAINING_GUIDE.md                 # ì´ íŒŒì¼
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. ë‹¨ì¼ GPU í•™ìŠµ

```bash
cd /home/irom/NAS/VLA/Insertion_VLAv4/Train
bash train_single_gpu.sh
```

### 2. ë©€í‹° GPU í•™ìŠµ (í˜„ì¬ ì‹œìŠ¤í…œ: GPU 1ê°œ)

```bash
# ëª¨ë“  GPU ì‚¬ìš©
bash train_multi_gpu.sh

# íŠ¹ì • GPUë§Œ ì‚¬ìš©
CUDA_VISIBLE_DEVICES=0,1 bash train_multi_gpu.sh
```

### 3. ë°±ê·¸ë¼ìš´ë“œ í•™ìŠµ (ì¥ì‹œê°„ í•™ìŠµ)

```bash
# ë‹¨ì¼ GPU
bash train_background.sh single

# ë©€í‹° GPU
bash train_background.sh multi

# ë¡œê·¸ í™•ì¸
tail -f outputs/train/smolvla_new_dataset/logs/train_*.log
```

### 4. í•™ìŠµ ì¤‘ë‹¨

```bash
bash stop_training.sh
```

## âš™ï¸ ì„¤ì • íŒŒì¼ (train_config_new_dataset.yaml)

### ì£¼ìš” ì„¤ì • í•­ëª©

```yaml
# ë°ì´í„°ì…‹
dataset:
  root_dir: "/home/irom/NAS/VLA/Insertion_VLAv4/New_dataset/collected_data"
  horizon: 1                    # ì•¡ì…˜ ì˜ˆì¸¡ í˜¸ë¼ì´ì¦Œ
  use_ee_pose: true            # EE pose ì‚¬ìš© (6ì°¨ì›)
  use_qpos: false              # Joint position ì‚¬ìš© ì•ˆ í•¨

# ì •ì±… ëª¨ë¸
policy:
  pretrained_model_path: "/home/irom/NAS/VLA/Insertion_VLAv4/sub_tasks/downloads/model"
  freeze_vision_encoder: true   # ë¹„ì „ ì¸ì½”ë” ë™ê²° (ë¹ ë¥¸ í•™ìŠµ)
  train_expert_only: true       # Expertë§Œ í•™ìŠµ (VLM ë™ê²°)
  use_multi_gpu: false          # ë©€í‹° GPU ì‚¬ìš© (ì‰˜ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ìë™ ì„¤ì •)

# í•™ìŠµ
training:
  steps: 50000                  # í•™ìŠµ ìŠ¤í…
  batch_size: 8                 # ë°°ì¹˜ í¬ê¸°
  log_freq: 100                 # ë¡œê·¸ ì¶œë ¥ ë¹ˆë„
  save_freq: 2000               # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë¹ˆë„

# ì˜µí‹°ë§ˆì´ì €
optimizer:
  lr: 1e-4                      # í•™ìŠµë¥ 
```

## ğŸ’» ë©€í‹° GPU ì„¤ì •

### DataParallel ì‚¬ìš©

í˜„ì¬ ì½”ë“œëŠ” PyTorchì˜ `DataParallel`ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:

```python
# train_smolvla_new_dataset.pyì—ì„œ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë¨
if use_multi_gpu and torch.cuda.device_count() > 1:
    policy = nn.DataParallel(policy)
```

### íš¨ê³¼ì ì¸ ë°°ì¹˜ í¬ê¸°

- **ë‹¨ì¼ GPU**: ë°°ì¹˜ í¬ê¸° = ì„¤ì • ê°’ (ì˜ˆ: 8)
- **ë©€í‹° GPU**: íš¨ê³¼ì ì¸ ë°°ì¹˜ í¬ê¸° = ì„¤ì • ê°’ Ã— GPU ê°œìˆ˜
  - ì˜ˆ: ë°°ì¹˜ í¬ê¸° 8, GPU 2ê°œ â†’ íš¨ê³¼ì ì¸ ë°°ì¹˜ í¬ê¸° 16

### GPU ë©”ëª¨ë¦¬ ê³ ë ¤ì‚¬í•­

í˜„ì¬ ì‹œìŠ¤í…œ: RTX 5080 (16GB VRAM)

| ë°°ì¹˜ í¬ê¸° | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ì˜ˆìƒ) | ê¶Œì¥ |
|----------|-------------------|------|
| 4        | ~8GB             | âœ… ì•ˆì „ |
| 8        | ~12GB            | âœ… ê¶Œì¥ |
| 16       | ~20GB            | âŒ OOM ìœ„í—˜ |

## ğŸ“Š í•™ìŠµ ëª¨ë‹ˆí„°ë§

### 1. ë¡œê·¸ í™•ì¸

```bash
# ì‹¤ì‹œê°„ ë¡œê·¸
tail -f outputs/train/smolvla_new_dataset/logs/train_*.log

# ìµœê·¼ ë¡œê·¸ í™•ì¸
ls -lht outputs/train/smolvla_new_dataset/logs/
```

### 2. GPU ì‚¬ìš©ëŸ‰ í™•ì¸

```bash
# 1ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸
watch -n 1 nvidia-smi

# GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ë§Œ í™•ì¸
nvidia-smi --query-gpu=index,name,memory.used,memory.total --format=csv
```

### 3. í•™ìŠµ ì§„í–‰ í™•ì¸

```bash
# í”„ë¡œì„¸ìŠ¤ í™•ì¸
ps aux | grep train_smolvla

# PID íŒŒì¼ í™•ì¸
cat outputs/train/smolvla_new_dataset/train.pid
```

### 4. ì²´í¬í¬ì¸íŠ¸ í™•ì¸

```bash
ls -lh outputs/train/smolvla_new_dataset/checkpoints/
```

## ğŸ”§ ê³ ê¸‰ ì˜µì…˜

### ëª…ë ¹ì¤„ ì˜µì…˜ìœ¼ë¡œ ì„¤ì • ë³€ê²½

```bash
# ë°°ì¹˜ í¬ê¸° ë³€ê²½
python train_smolvla_new_dataset.py --config train_config_new_dataset.yaml --batch_size 4

# í•™ìŠµ ìŠ¤í… ë³€ê²½
python train_smolvla_new_dataset.py --config train_config_new_dataset.yaml --steps 100000

# Learning rate ë³€ê²½
python train_smolvla_new_dataset.py --config train_config_new_dataset.yaml --lr 5e-5

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ë³€ê²½
python train_smolvla_new_dataset.py --config train_config_new_dataset.yaml --output_dir outputs/my_training

# ì—¬ëŸ¬ ì˜µì…˜ ë™ì‹œ ì‚¬ìš©
python train_smolvla_new_dataset.py \
    --config train_config_new_dataset.yaml \
    --batch_size 4 \
    --steps 100000 \
    --lr 5e-5
```

### íŠ¹ì • GPU ì„ íƒ

```bash
# GPU 0ë§Œ ì‚¬ìš©
CUDA_VISIBLE_DEVICES=0 bash train_single_gpu.sh

# GPU 1ë§Œ ì‚¬ìš©
CUDA_VISIBLE_DEVICES=1 bash train_single_gpu.sh

# GPU 0,1,2 ì‚¬ìš©
CUDA_VISIBLE_DEVICES=0,1,2 bash train_multi_gpu.sh
```

## ğŸ“ˆ í•™ìŠµ ê²°ê³¼

### ì²´í¬í¬ì¸íŠ¸

```
outputs/train/smolvla_new_dataset/checkpoints/
â”œâ”€â”€ checkpoint_step_0002000.pt
â”œâ”€â”€ checkpoint_step_0004000.pt
â”œâ”€â”€ checkpoint_step_0006000.pt
â”œâ”€â”€ ...
â””â”€â”€ checkpoint_latest.pt
```

### ìµœì¢… ëª¨ë¸

```
outputs/train/smolvla_new_dataset/final_model/
â”œâ”€â”€ config.json
â”œâ”€â”€ pytorch_model.bin
â””â”€â”€ ...
```

## ğŸ› ë¬¸ì œ í•´ê²°

### OOM (Out of Memory) ì—ëŸ¬

```bash
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
python train_smolvla_new_dataset.py --config train_config_new_dataset.yaml --batch_size 4

# ë˜ëŠ” ì„¤ì • íŒŒì¼ ìˆ˜ì •
vim train_config_new_dataset.yaml
# training.batch_sizeë¥¼ 4ë¡œ ë³€ê²½
```

### í•™ìŠµì´ ëŠë¦° ê²½ìš°

```yaml
# train_config_new_dataset.yaml ìˆ˜ì •
training:
  num_workers: 8  # ë°ì´í„° ë¡œë”© ì›Œì»¤ ì¦ê°€ (CPU ì½”ì–´ ìˆ˜ì— ë§ê²Œ ì¡°ì •)
```

### ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ

```python
# ìˆ˜ë™ìœ¼ë¡œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (train_smolvla_new_dataset.py ìˆ˜ì • í•„ìš”)
checkpoint = torch.load("outputs/train/smolvla_new_dataset/checkpoints/checkpoint_latest.pt")
policy.load_state_dict(checkpoint["policy_state_dict"])
optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
scheduler.load_state_dict(checkpoint["scheduler_state_dict"])
start_step = checkpoint["step"]
```

## ğŸ“ í•™ìŠµ íŒ

1. **ì²˜ìŒì—ëŠ” ì§§ê²Œ í…ŒìŠ¤íŠ¸**
   ```bash
   python train_smolvla_new_dataset.py --config train_config_new_dataset.yaml --steps 1000
   ```

2. **ë°ì´í„° í™•ì¸**
   ```bash
   python hdf5_lerobot_adapter.py
   ```

3. **GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§**
   ```bash
   watch -n 1 nvidia-smi
   ```

4. **ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ì¶”ì²œ** (ì¥ì‹œê°„ í•™ìŠµ)
   ```bash
   bash train_background.sh single
   ```

5. **ì •ê¸°ì ìœ¼ë¡œ ì²´í¬í¬ì¸íŠ¸ í™•ì¸**
   - ê¸°ë³¸: 2000 ìŠ¤í…ë§ˆë‹¤ ì €ì¥
   - í•„ìš”ì‹œ `save_freq` ì¡°ì •

## ğŸ“ ë„ì›€ë§

ë¬¸ì œê°€ ë°œìƒí•˜ë©´:

1. ë¡œê·¸ íŒŒì¼ í™•ì¸: `outputs/train/smolvla_new_dataset/logs/`
2. GPU ìƒíƒœ í™•ì¸: `nvidia-smi`
3. ì„¤ì • íŒŒì¼ í™•ì¸: `train_config_new_dataset.yaml`
4. ë°ì´í„° í™•ì¸: `python hdf5_lerobot_adapter.py`

---

**Created**: 2025-12-23
**Dataset**: New HDF5 VLA Dataset (18 episodes)
**Model**: SmolVLA (downloaded from Hugging Face)
