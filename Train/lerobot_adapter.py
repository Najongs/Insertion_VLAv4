"""
LeRobot Adapter for VLA Dataset

This module provides an adapter to convert VLA dataset format to LeRobot format
for training SmolVLA and other vision-language-action models.
"""

import torch
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Union
from torch.utils.data import Dataset, ConcatDataset
from PIL import Image
import logging

from vla_dataset import VLADataset

logger = logging.getLogger(__name__)


class VLALeRobotDataset(Dataset):
    """
    Adapter that wraps VLADataset to provide LeRobot-compatible samples.

    LeRobot expects samples with the following structure:
    {
        "observation.images.camera1": Tensor,  # (C, H, W) - channel first!
        "observation.images.camera2": Tensor,  # (C, H, W)
        "observation.state": Tensor,           # (state_dim,)
        "action": Tensor,                      # (action_dim,)
        "task": str,                           # Task instruction
        "timestamp": float,
        "frame_index": int,
        "episode_index": int,
        "index": int,
    }
    """

    def __init__(
        self,
        episode_dir: str,
        episode_index: int = 0,
        horizon: int = 8,
        sensor_window_size: int = 65,
        robot_window_size: int = 100,
        action_expert_hz: int = 10,
        use_joints_only: bool = False,
        use_poses_only: bool = True,
        use_full_action_chunk: bool = False,
    ):
        """
        Args:
            episode_dir: Path to episode directory (contains metadata.json)
            episode_index: Episode index for this dataset
            horizon: Action prediction horizon
            sensor_window_size: Sensor history window size
            robot_window_size: Robot state history window size
            action_expert_hz: Action frequency in Hz
            use_joints_only: Use only joint angles (6 dims) for state
            use_poses_only: Use only poses (6 dims) for state (default)
            use_full_action_chunk: Use full action horizon instead of first action only
        """
        super().__init__()

        self.episode_dir = Path(episode_dir)
        self.episode_index = episode_index
        self.use_joints_only = use_joints_only
        self.use_poses_only = use_poses_only
        self.use_full_action_chunk = use_full_action_chunk

        # Create underlying VLA dataset
        self.vla_dataset = VLADataset(
            data_dir=str(episode_dir),
            horizon=horizon,
            sensor_window_size=sensor_window_size,
            robot_window_size=robot_window_size,
            action_expert_hz=action_expert_hz,
        )

        # Extract task instruction (use the instruction generated by VLADataset)
        self.task = self.vla_dataset.instruction

        # Get number of available camera views
        self.num_cameras = len(self.vla_dataset.images)

        logger.info(f"Loaded VLA episode: {self.episode_dir.name}")
        logger.info(f"  Episode index: {self.episode_index}")
        logger.info(f"  Total frames: {len(self.vla_dataset)}")
        logger.info(f"  Task: {self.task}")
        logger.info(f"  Cameras: {self.num_cameras}")
        logger.info(f"  State mode: {'joints_only' if use_joints_only else 'poses_only' if use_poses_only else 'full'}")
        logger.info(f"  Action mode: {'full_chunk' if use_full_action_chunk else 'single_step'}")

    def __len__(self) -> int:
        return len(self.vla_dataset)

    def __getitem__(self, idx: int) -> Dict:
        """
        Get a sample in LeRobot format.

        Returns:
            Dictionary with LeRobot-compatible keys and values.
        """
        # Get sample from VLA dataset
        vla_sample = self.vla_dataset[idx]

        # Create LeRobot sample
        lerobot_sample = {
            # Metadata
            "task": self.task,
            "timestamp": vla_sample["timestamp"],
            "frame_index": idx,
            "episode_index": self.episode_index,
            "index": idx,  # Will be updated by ConcatDataset
        }

        # Process images: Load from paths and convert to (C, H, W) tensors
        for cam_idx, img_path in enumerate(vla_sample["images"], start=1):
            try:
                # Load image as PIL Image
                img = Image.open(img_path).convert("RGB")

                # Resize to standard size (480x640) to avoid size mismatch issues
                # Use LANCZOS for high-quality downsampling
                img = img.resize((640, 480), Image.Resampling.LANCZOS)

                # Convert to numpy array and normalize to [0, 1]
                img_np = np.array(img, dtype=np.float32) / 255.0

                # Convert to tensor: (H, W, C) -> (C, H, W)
                img_tensor = torch.from_numpy(img_np).permute(2, 0, 1).contiguous()

                # Add to sample with LeRobot naming convention
                lerobot_sample[f"observation.images.camera{cam_idx}"] = img_tensor

            except Exception as e:
                logger.warning(f"Failed to load image {img_path}: {e}")
                # Create dummy image if loading fails
                lerobot_sample[f"observation.images.camera{cam_idx}"] = torch.zeros(3, 480, 640)

        # Process robot state
        # vla_sample["robot_states"] has shape (robot_window_size, 12)
        # where 12 = 6 joints + 6 poses
        # Take the last timestep as current state
        robot_state = vla_sample["robot_states"][-1]  # Shape: (12,)

        if self.use_joints_only:
            # Use only joint angles (first 6 dims)
            state = robot_state[:6]
        elif self.use_poses_only:
            # Use only poses (last 6 dims)
            state = robot_state[6:]
        else:
            # Use full state (12 dims)
            state = robot_state

        lerobot_sample["observation.state"] = state

        # Process actions
        # vla_sample["actions"] has shape (horizon, 7)
        # where 7 = delta_xyz (3) + delta_rotation (3) + gripper (1)
        if self.use_full_action_chunk:
            # Use full action horizon
            lerobot_sample["action"] = vla_sample["actions"]  # Shape: (horizon, 7)
        else:
            # Use only first action
            lerobot_sample["action"] = vla_sample["actions"][0]  # Shape: (7,)

        return lerobot_sample


def create_vla_lerobot_dataset(
    episode_dirs: List[Union[str, Path]],
    horizon: int = 8,
    sensor_window_size: int = 65,
    robot_window_size: int = 100,
    action_expert_hz: int = 10,
    use_joints_only: bool = False,
    use_poses_only: bool = True,
    use_full_action_chunk: bool = False,
) -> Dataset:
    """
    Create a combined VLA LeRobot dataset from multiple episodes.

    Args:
        episode_dirs: List of episode directory paths
        horizon: Action prediction horizon
        sensor_window_size: Sensor history window size
        robot_window_size: Robot state history window size
        action_expert_hz: Action frequency in Hz
        use_joints_only: Use only joint angles for state
        use_poses_only: Use only poses for state (default)
        use_full_action_chunk: Use full action horizon

    Returns:
        Combined dataset (ConcatDataset if multiple episodes)
    """
    datasets = []

    for episode_idx, episode_dir in enumerate(episode_dirs):
        episode_path = Path(episode_dir)

        # Check if directory exists and has metadata
        if not episode_path.exists():
            logger.warning(f"Episode directory does not exist: {episode_path}")
            continue

        metadata_path = episode_path / "metadata.json"
        if not metadata_path.exists():
            logger.warning(f"No metadata.json found in: {episode_path}")
            continue

        # Create dataset for this episode
        try:
            dataset = VLALeRobotDataset(
                episode_dir=str(episode_path),
                episode_index=episode_idx,
                horizon=horizon,
                sensor_window_size=sensor_window_size,
                robot_window_size=robot_window_size,
                action_expert_hz=action_expert_hz,
                use_joints_only=use_joints_only,
                use_poses_only=use_poses_only,
                use_full_action_chunk=use_full_action_chunk,
            )
            datasets.append(dataset)
        except Exception as e:
            logger.error(f"Failed to load episode {episode_path}: {e}")
            continue

    if len(datasets) == 0:
        raise ValueError("No valid episodes found!")

    logger.info(f"Created combined dataset with {len(datasets)} episodes")

    # Combine all episodes
    if len(datasets) == 1:
        combined_dataset = datasets[0]
    else:
        combined_dataset = ConcatDataset(datasets)

    # Update global indices
    global_idx = 0
    for dataset in datasets:
        for i in range(len(dataset)):
            # Note: We can't directly modify samples in ConcatDataset
            # The global index will be managed by the dataloader
            global_idx += 1

    logger.info(f"Total samples: {len(combined_dataset)}")

    return combined_dataset


def lerobot_collate_fn(batch: List[Dict]) -> Dict:
    """
    Custom collate function for LeRobot batches.

    Handles:
    - Stacking tensors (images, state, actions)
    - Keeping lists (task strings)
    - Maintaining metadata

    Args:
        batch: List of samples from VLALeRobotDataset

    Returns:
        Batched dictionary
    """
    if len(batch) == 0:
        return {}

    # Get all keys from first sample
    keys = batch[0].keys()

    batched = {}

    for key in keys:
        values = [sample[key] for sample in batch]

        # Handle different types
        if key == "task":
            # Keep as list of strings
            batched[key] = values
        elif key in ["timestamp", "frame_index", "episode_index", "index"]:
            # Stack scalars into tensor
            batched[key] = torch.tensor(values)
        elif isinstance(values[0], torch.Tensor):
            # Stack tensors
            batched[key] = torch.stack(values, dim=0)
        else:
            # Keep as list for other types
            batched[key] = values

    return batched


if __name__ == "__main__":
    """
    Test the VLA LeRobot adapter.
    """
    import logging
    logging.basicConfig(level=logging.INFO)

    print("üß™ Testing VLA LeRobot Adapter...\n")

    # Test with a single episode
    test_episode = "/home/najo/NAS/VLA/dataset/New_dataset2/Green_point/data_collection_20251108_053848"

    try:
        dataset = VLALeRobotDataset(
            episode_dir=test_episode,
            episode_index=0,
            use_poses_only=True,
        )

        print(f"‚úÖ Dataset created: {len(dataset)} samples\n")

        # Test first sample
        sample = dataset[0]

        print("üìä Sample structure:")
        for key, value in sample.items():
            if isinstance(value, torch.Tensor):
                print(f"  {key}: {value.shape} ({value.dtype})")
            else:
                print(f"  {key}: {type(value).__name__} = {value if not isinstance(value, str) else value[:50]}")

        print("\n‚úÖ Sample loaded successfully!\n")

        # Test dataloader
        from torch.utils.data import DataLoader

        dataloader = DataLoader(
            dataset,
            batch_size=4,
            shuffle=False,
            collate_fn=lerobot_collate_fn,
            num_workers=0,
        )

        batch = next(iter(dataloader))

        print("üì¶ Batch structure:")
        for key, value in batch.items():
            if isinstance(value, torch.Tensor):
                print(f"  {key}: {value.shape} ({value.dtype})")
            elif isinstance(value, list):
                print(f"  {key}: List[{type(value[0]).__name__}] (len={len(value)})")

        print("\n‚úÖ DataLoader works!\n")

        # Test with multiple episodes
        print("Testing with multiple episodes...")

        episode_dirs = [
            "/home/najo/NAS/VLA/dataset/New_dataset2/Green_point/data_collection_20251108_053848",
            "/home/najo/NAS/VLA/dataset/New_dataset2/Green_point/data_collection_20251108_060157",
        ]

        multi_dataset = create_vla_lerobot_dataset(
            episode_dirs=episode_dirs,
            use_poses_only=True,
        )

        print(f"\n‚úÖ Multi-episode dataset: {len(multi_dataset)} samples")

        print("\nüéâ All tests passed!")

    except Exception as e:
        print(f"\n‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()
