# VLA Dataset for Insertion Task

í†µí•© ë°ì´í„°ì…‹ ëª¨ë“ˆ - ë¡œë´‡ insertion ì‘ì—…ì„ ìœ„í•œ PyTorch Dataset

## ğŸ“ ë°ì´í„° êµ¬ì¡°

```
/home/najo/NAS/VLA/dataset/
â”œâ”€â”€ New_dataset2/
â”‚   â”œâ”€â”€ Green_point/
â”‚   â”‚   â”œâ”€â”€ data_collection_20251108_053848/
â”‚   â”‚   â”‚   â”œâ”€â”€ metadata.json          # ë©”íƒ€ë°ì´í„°
â”‚   â”‚   â”‚   â”œâ”€â”€ robot_states.npz       # ë¡œë´‡ ìƒíƒœ (joints + poses)
â”‚   â”‚   â”‚   â”œâ”€â”€ sensor_data_*.npz      # ì„¼ì„œ ë°ì´í„° (OCT alines + forces)
â”‚   â”‚   â”‚   â”œâ”€â”€ View1/*.jpg            # ì¹´ë©”ë¼ ë·° 1
â”‚   â”‚   â”‚   â”œâ”€â”€ View2/*.jpg            # ì¹´ë©”ë¼ ë·° 2
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ Red_point/
â”‚   â””â”€â”€ Blue_point/
â”œâ”€â”€ New_dataset3/
â”œâ”€â”€ New_dataset4/
â”œâ”€â”€ New_dataset5/
â””â”€â”€ New_dataset6/
```

## ğŸš€ ì‚¬ìš©ë²•

### ê¸°ë³¸ ì‚¬ìš© ì˜ˆì œ

```python
from vla_dataset import VLADataset, create_dataloader

# ë‹¨ì¼ ì—í”¼ì†Œë“œ ë¡œë“œ
dataset = VLADataset(
    data_dir="/home/najo/NAS/VLA/dataset/New_dataset2/Green_point/data_collection_20251108_053848",
    horizon=8,
    sensor_window_size=65,
    robot_window_size=100,
    action_expert_hz=10,
)

print(f"Dataset size: {len(dataset)}")

# ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸°
sample = dataset[0]
print(f"Images: {len(sample['images'])} views")
print(f"Sensor data: {sample['sensor_data'].shape}")  # (65, 1026)
print(f"Robot states: {sample['robot_states'].shape}")  # (100, 12)
print(f"Actions: {sample['actions'].shape}")  # (8, 7)
```

### DataLoader ìƒì„±

```python
# ì—¬ëŸ¬ íƒœìŠ¤í¬ì˜ ì—í”¼ì†Œë“œë“¤ì„ ìë™ìœ¼ë¡œ ë¡œë“œ
dataloader = create_dataloader(
    dataset_paths=[
        "/home/najo/NAS/VLA/dataset/New_dataset2/Green_point",
        "/home/najo/NAS/VLA/dataset/New_dataset2/Red_point",
        "/home/najo/NAS/VLA/dataset/New_dataset2/Blue_point",
    ],
    batch_size=4,
    num_workers=4,
    shuffle=True,
    horizon=8,
    sensor_window_size=65,
    robot_window_size=100,
    action_expert_hz=10,
)

# í•™ìŠµ ë£¨í”„
for batch in dataloader:
    instructions = batch['instruction']  # List[str]
    images = batch['images']  # List[List[str]]
    sensor_data = batch['sensor_data']  # (B, T_sensor, 1026)
    robot_states = batch['robot_states']  # (B, T_robot, 12)
    actions = batch['actions']  # (B, horizon, 7)

    # ëª¨ë¸ í•™ìŠµ...
```

## ğŸ“Š ë°ì´í„° í˜•ì‹

### Sample êµ¬ì¡°

```python
{
    'instruction': str,              # íƒœìŠ¤í¬ instruction
    'images': List[str],            # ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (ê° viewë§ˆë‹¤)
    'sensor_data': Tensor,          # Shape: (sensor_window_size, 1026)
                                    #   1026 = 1025 (OCT alines) + 1 (force)
    'robot_states': Tensor,         # Shape: (robot_window_size, 12)
                                    #   12 = 6 (joints) + 6 (poses)
    'actions': Tensor,              # Shape: (horizon, 7)
                                    #   7 = 3 (delta_xyz) + 3 (delta_rotation) + 1 (gripper)
    'has_sensor': bool,             # ì„¼ì„œ ë°ì´í„° ìœ íš¨ ì—¬ë¶€
    'has_robot_states': bool,       # ë¡œë´‡ ìƒíƒœ ìœ íš¨ ì—¬ë¶€
    'episode_id': str,              # ì—í”¼ì†Œë“œ ID
    'timestamp': float,             # íƒ€ì„ìŠ¤íƒ¬í”„
}
```

### Batch êµ¬ì¡°

```python
{
    'instruction': List[str],                    # (B,)
    'images': List[List[str]],                   # (B, num_views)
    'sensor_data': Tensor,                       # (B, sensor_window_size, 1026)
    'robot_states': Tensor,                      # (B, robot_window_size, 12)
    'actions': Tensor,                           # (B, horizon, 7)
    'has_sensor_mask': BoolTensor,              # (B,)
    'has_robot_states_mask': BoolTensor,        # (B,)
    'episode_ids': List[str],                    # (B,)
    'timestamps': List[float],                   # (B,)
}
```

## âš™ï¸ íŒŒë¼ë¯¸í„° ì„¤ëª…

### VLADataset

- `data_dir`: ì—í”¼ì†Œë“œ ë””ë ‰í† ë¦¬ ê²½ë¡œ (metadata.json í¬í•¨)
- `horizon`: Action prediction horizon (default: 8)
  - í•œ ë²ˆì— ì˜ˆì¸¡í•  ë¯¸ë˜ actionì˜ ê°œìˆ˜
- `sensor_window_size`: ì„¼ì„œ íˆìŠ¤í† ë¦¬ ìœˆë„ìš° í¬ê¸° (default: 65)
  - ê³¼ê±° 65ê°œì˜ ì„¼ì„œ ë°ì´í„° ì‚¬ìš© (trailing window)
- `robot_window_size`: ë¡œë´‡ ìƒíƒœ íˆìŠ¤í† ë¦¬ ìœˆë„ìš° í¬ê¸° (default: 100)
  - ê³¼ê±° 100ê°œì˜ ë¡œë´‡ ìƒíƒœ ì‚¬ìš© (trailing window)
- `action_expert_hz`: Action frequency in Hz (default: 10)
  - ë¡œë´‡ì€ 100Hzë¡œ ì›€ì§ì´ì§€ë§Œ, actionì€ 10Hzë¡œ ìƒì„±
  - action_interval = robot_hz / action_expert_hz = 10

### create_dataloader

- `dataset_paths`: íƒœìŠ¤í¬ ë””ë ‰í† ë¦¬ ë˜ëŠ” ì—í”¼ì†Œë“œ ë””ë ‰í† ë¦¬ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
- `batch_size`: ë°°ì¹˜ í¬ê¸° (default: 4)
- `num_workers`: DataLoader worker ìˆ˜ (default: 4)
- `shuffle`: ë°ì´í„° ì„ê¸° ì—¬ë¶€ (default: True)

## ğŸ”§ ì£¼ìš” íŠ¹ì§•

1. **ë©”ëª¨ë¦¬ ìµœì í™”**
   - `mmap_mode='r'`ì„ ì‚¬ìš©í•œ lazy loading
   - ì‹¤ì œ í•„ìš”í•  ë•Œë§Œ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œ

2. **Trailing Window**
   - ì„¼ì„œ ë°ì´í„°ì™€ ë¡œë´‡ ìƒíƒœëŠ” **ê³¼ê±° ë°ì´í„°ë§Œ** ì‚¬ìš©
   - ì‹¤ì œ inference ìƒí™©ê³¼ ë™ì¼í•˜ê²Œ ë¯¸ë˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ

3. **Delta Action**
   - Actionì€ absolute poseê°€ ì•„ë‹Œ **delta pose**ë¡œ ê³„ì‚°
   - Translation: `end_pose[:3] - start_pose[:3]`
   - Rotation: rotation vectorë¡œ í‘œí˜„ëœ delta rotation

4. **Terminal Action**
   - ì—í”¼ì†Œë“œ ë 5ê°œ actionì€ ì •ì§€ ì‹ í˜¸ (ëª¨ë‘ 0)

5. **Multi-view Support**
   - View1 ~ View5ê¹Œì§€ ìµœëŒ€ 5ê°œì˜ ì¹´ë©”ë¼ ë·° ì§€ì›

## ğŸ“¦ ì˜ì¡´ì„±

```bash
pip install -r requirements.txt
```

í•„ìš”í•œ íŒ¨í‚¤ì§€:
- torch >= 2.0.0
- numpy >= 1.24.0
- scipy >= 1.10.0
- pandas >= 2.0.0
- Pillow >= 9.0.0

## ğŸ¤– SmolVLA í•™ìŠµ

### í•™ìŠµ ì¤€ë¹„

VLA ë°ì´í„°ì…‹ìœ¼ë¡œ SmolVLA ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ íŒŒì¼ë“¤ì´ ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

1. **lerobot_adapter.py** - VLA ë°ì´í„°ì…‹ì„ LeRobot í˜•ì‹ìœ¼ë¡œ ë³€í™˜
2. **train_config.yaml** - SmolVLA í•™ìŠµ ì„¤ì •
3. **train_smolvla.py** - í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

### ë¹ ë¥¸ ì‹œì‘

```bash
cd /home/najo/NAS/VLA/Insertion_VLAv4/Train

# 1. ë°ì´í„°ì…‹ ì–´ëŒ‘í„° í…ŒìŠ¤íŠ¸
python lerobot_adapter.py

# 2. í•™ìŠµ ì‹œì‘ (ê¸°ë³¸ ì„¤ì •)
python train_smolvla.py --config train_config.yaml

# 3. ì»¤ìŠ¤í…€ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ
python train_smolvla.py \
  --config train_config.yaml \
  --batch_size 4 \
  --steps 10000 \
  --lr 5e-5
```

### í•™ìŠµ ì„¤ì • ìˆ˜ì •

`train_config.yaml` íŒŒì¼ì—ì„œ ì£¼ìš” ì„¤ì •ì„ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```yaml
# ë°ì´í„°ì…‹ ì—í”¼ì†Œë“œ ì¶”ê°€/ì œê±°
dataset:
  episode_dirs:
    - "New_dataset2/Green_point/data_collection_20251108_053719"
    - "New_dataset2/Green_point/data_collection_20251108_053848"
    # ... ë” ë§ì€ ì—í”¼ì†Œë“œ ì¶”ê°€

# í•™ìŠµ íŒŒë¼ë¯¸í„° ì¡°ì •
training:
  batch_size: 8          # GPU ë©”ëª¨ë¦¬ì— ë§ê²Œ ì¡°ì •
  steps: 20000           # í•™ìŠµ ìŠ¤í… ìˆ˜
  log_freq: 100          # ë¡œê·¸ ì¶œë ¥ ë¹ˆë„
  save_freq: 2000        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë¹ˆë„

# ìµœì í™” ì„¤ì •
optimizer:
  lr: 1e-4               # í•™ìŠµë¥ 
```

### í•™ìŠµ ëª¨ë‹ˆí„°ë§

í•™ìŠµ ì¤‘ ë‹¤ìŒ ì •ë³´ê°€ ì¶œë ¥ë©ë‹ˆë‹¤:
- Loss ê°’
- Learning rate
- í•™ìŠµ ì‹œê°„

ì²´í¬í¬ì¸íŠ¸ëŠ” `outputs/train/smolvla_vla_insertion/checkpoints/`ì— ì €ì¥ë©ë‹ˆë‹¤.

### í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©

í•™ìŠµì´ ì™„ë£Œë˜ë©´ `outputs/train/smolvla_vla_insertion/final_model/`ì— ìµœì¢… ëª¨ë¸ì´ ì €ì¥ë©ë‹ˆë‹¤.

ì´ ëª¨ë¸ì„ inference ì½”ë“œì—ì„œ ì‚¬ìš©:

```python
from lerobot.policies.smolvla.modeling_smolvla import SmolVLAPolicy

# í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
policy = SmolVLAPolicy.from_pretrained(
    "outputs/train/smolvla_vla_insertion/final_model"
)
policy.eval()

# ì¶”ë¡  ì‹¤í–‰
action = policy.select_action(observation)
```

## ğŸ§ª í…ŒìŠ¤íŠ¸

```bash
cd /home/najo/NAS/VLA/Insertion_VLAv4/Train

# VLA ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸
python vla_dataset.py

# LeRobot ì–´ëŒ‘í„° í…ŒìŠ¤íŠ¸
python lerobot_adapter.py
```

ì˜ˆìƒ ì¶œë ¥:
```
ğŸ§ª Testing VLA Dataset...
âœ… Dataset created: 710 samples
âœ… All tests passed!
```

## ğŸ“ ì°¸ê³ ì‚¬í•­

### Action ê³„ì‚°

Actionì€ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ë©ë‹ˆë‹¤:

```python
# 10Hz action from 100Hz robot states
action_interval = 10  # 100Hz / 10Hz

for i in range(horizon):
    start_idx = (action_step + i) * action_interval
    end_idx = start_idx + action_interval

    # Delta translation
    delta_xyz = poses[end_idx][:3] - poses[start_idx][:3]

    # Delta rotation (rotation vector)
    r_start = Rotation.from_euler("xyz", poses[start_idx][3:], degrees=True)
    r_end = Rotation.from_euler("xyz", poses[end_idx][3:], degrees=True)
    delta_rotation = (r_end * r_start.inv()).as_rotvec()

    # Combine: [dx, dy, dz, drx, dry, drz, gripper]
    action = [*delta_xyz, *delta_rotation, 1.0]
```

### Sensor Data

ì„¼ì„œ ë°ì´í„°ëŠ” OCT A-lineê³¼ Force ì„¼ì„œë¥¼ ê²°í•©:

```python
# Shape: (sensor_window_size, 1026)
#   - alines: (sensor_window_size, 1025)
#   - forces: (sensor_window_size, 1)
sensor_data = np.concatenate([alines, forces[:, None]], axis=1)
```

ì„¼ì„œëŠ” 650Hzë¡œ ìƒ˜í”Œë§ë˜ë©°, ë¡œë´‡ 100Hzì— ëŒ€ì‘í•˜ì—¬ ë™ê¸°í™”:
```python
sensor_ratio = 650 / 100 = 6.5
sensor_idx = robot_idx * 6.5
```
