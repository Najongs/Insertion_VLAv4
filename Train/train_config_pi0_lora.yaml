# Training Configuration for π0 (Pi Zero) with LoRA
# ===================================================
# π0 is Physical Intelligence's flow-matching VLA model
# LoRA (Low-Rank Adaptation) enables parameter-efficient fine-tuning
#
# Memory Requirements:
# - Full Fine-Tuning: > 70 GB per GPU ❌
# - LoRA Fine-Tuning: > 22.5 GB per GPU ✅ (works on RTX 3090!)
#
# LoRA Benefits:
# - Only ~2-5% of parameters trained
# - Much lower memory usage
# - Faster training
# - Smaller checkpoints (100-500 MB vs 5-10 GB)

# Random seed for reproducibility
seed: 1000

# Output directory for checkpoints and logs
output_dir: "outputs/train/pi0_lora_needle_insertion"

# Dataset Configuration
# ---------------------
dataset:
  # Root directory containing HDF5 episode files
  root_dir: "/home/najo/NAS/VLA/dataset/New_dataset/collected_data/Red_point"

  # Action prediction horizon (π0 uses action chunks)
  horizon: 50

  # State representation
  use_qpos: false        # Use joint positions (6 dims)
  use_ee_pose: true      # Use end-effector pose (6 dims) - default

  # Task instruction (π0 uses language conditioning)
  task_instruction: "Insert needle into red point"

  # Data Augmentation
  # -----------------
  augment: true
  augment_brightness: 0.15       # Max brightness change (±15%)
  augment_contrast: 0.15         # Max contrast change (±15%)
  augment_saturation: 0.10       # Saturation change (±10%)
  augment_hue: 0.05              # Hue shift (±18 degrees)
  augment_noise: 0.01            # Gaussian noise std (1%)

# Policy Configuration
# --------------------
policy:
  # π0 configuration - Using gemma_2b with LoRA for efficiency
  pretrained_model_id: null  # No pretrained model for now

  # Observation and action
  n_obs_steps: 1         # Number of observation steps (π0 uses single observation)
  chunk_size: 50         # Action chunk size (action horizon)
  n_action_steps: 50     # Number of action steps to execute

  # Model architecture - gemma_2b works with LoRA on RTX 3090!
  paligemma_variant: "gemma_2b"        # Full PaliGemma 2B (LoRA makes it fit!)
  action_expert_variant: "gemma_300m"  # Action expert variant
  dtype: "bfloat16"                     # Data type (bfloat16 for memory efficiency)

  # Dimensions
  max_state_dim: 32      # Maximum state dimension (padded)
  max_action_dim: 32     # Maximum action dimension (padded)

  # Flow matching parameters
  num_inference_steps: 10              # Number of denoising steps
  time_sampling_beta_alpha: 1.5
  time_sampling_beta_beta: 1.0
  time_sampling_scale: 0.999
  time_sampling_offset: 0.001
  min_period: 0.004
  max_period: 4.0

  # Image resolution (π0 uses 224x224)
  image_resolution: [224, 224]

  # Training optimization
  gradient_checkpointing: true   # Enable for additional memory optimization
  compile_model: false           # Disable torch.compile
  tokenizer_max_length: 48       # PaliGemma tokenizer max length

# LoRA Configuration
# ------------------
lora:
  # LoRA rank (controls number of trainable parameters)
  # Higher rank = more parameters = better performance but more memory
  lora_r: 8              # Typical values: 4, 8, 16, 32

  # LoRA alpha (scaling factor)
  # Typically set to 2x the rank
  lora_alpha: 16         # Typical: 2 * lora_r

  # LoRA dropout (regularization)
  lora_dropout: 0.05     # Typical: 0.05-0.1

  # Target modules (which layers to apply LoRA)
  # Targeting attention and MLP layers for maximum impact
  target_modules:
    - "q_proj"           # Query projection
    - "v_proj"           # Value projection
    - "k_proj"           # Key projection
    - "o_proj"           # Output projection
    - "gate_proj"        # MLP gate projection
    - "up_proj"          # MLP up projection
    - "down_proj"        # MLP down projection

# Training Configuration
# ----------------------
training:
  # Training steps
  steps: 30000           # 30k steps (π0 typical training length)

  # Batch size
  batch_size: 4          # Per-GPU batch size (effective=20 with 5 GPUs)

  # DataLoader settings
  shuffle: true
  num_workers: 4
  pin_memory: true

  # Logging and saving
  log_freq: 100          # Log every N steps
  save_freq_epochs: 1.0  # Save checkpoint every N epochs

# Optimizer Configuration
# -----------------------
optimizer:
  lr: 0.000025           # Learning rate (2.5e-5)
  betas: [0.9, 0.95]     # Adam betas
  weight_decay: 0.01     # Weight decay
  eps: 0.00000001        # Adam epsilon (1e-8)
  grad_clip_norm: 1.0    # Gradient clipping norm

# Scheduler Configuration
# -----------------------
scheduler:
  num_warmup_steps: 1000      # Warmup steps
  num_decay_steps: 30000      # Decay steps (matches training steps)
  peak_lr: 0.000025           # Peak learning rate (2.5e-5)
  decay_lr: 0.0000025         # Final learning rate (2.5e-6)

# Notes:
# ------
# 1. LoRA only trains ~2-5% of parameters (10-50M vs 2.5B)
# 2. Memory usage: > 22.5 GB per GPU (fits on RTX 3090!)
# 3. Checkpoint size: ~100-500 MB (vs 5-10 GB full model)
# 4. Training speed: ~1-2 steps/sec (faster than full fine-tuning)
# 5. Performance: Slightly lower than full fine-tuning but still very good
# 6. Can train multiple LoRA adapters for different tasks
#
# LoRA Hyperparameter Tuning:
# - Increase lora_r (e.g., 16, 32) for better performance (more memory)
# - Decrease lora_r (e.g., 4) for lower memory (slightly worse performance)
# - lora_alpha typically 2x lora_r
# - lora_dropout 0.05-0.1 for regularization
#
# Performance expectations:
# - Similar to full fine-tuning for most tasks
# - Excellent for task-specific fine-tuning
# - Can merge adapters back into base model for inference
# - Ideal for experimenting with different tasks
