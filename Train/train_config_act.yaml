# Training Configuration for ACT (Action Chunking Transformer)
# ============================================================
# ACT is proven effective for insertion tasks (ALOHA peg insertion)
# This config is optimized for precise needle insertion

# Random seed for reproducibility
seed: 1000

# Output directory for checkpoints and logs
output_dir: "outputs/train/act_needle_insertion"

# Dataset Configuration
# ---------------------
dataset:
  # Root directory containing HDF5 episode files
  root_dir: "/home/najo/NAS/VLA/dataset/New_dataset/collected_data/Red_point"

  # Action prediction horizon (ACT typically uses larger chunks than VLA)
  # ACT paper uses 100 for ALOHA insertion tasks
  horizon: 100

  # State representation
  use_qpos: false        # Use joint positions (6 dims)
  use_ee_pose: true      # Use end-effector pose (6 dims) - default

  # Task instruction
  task_instruction: "Insert needle into red point"

  # Data Augmentation
  # -----------------
  # ACT is more robust to augmentation than VLA
  augment: true
  augment_brightness: 0.15       # Max brightness change (±15%)
  augment_contrast: 0.15         # Max contrast change (±15%)
  augment_saturation: 0.10       # Saturation change (±10%)
  augment_hue: 0.05              # Hue shift (±18 degrees, 0.05 * 360)
  augment_noise: 0.01            # Gaussian noise std (1%)

# Policy Configuration
# --------------------
policy:
  # Pretrained model specialized for insertion tasks
  pretrained_model_id: "lerobot/act_aloha_sim_insertion_human"

  # ACT configuration
  n_obs_steps: 1         # Number of observation steps (ACT typically uses 1)
  chunk_size: 100        # Action chunk size (same as horizon)
  n_action_steps: 100    # Number of action steps to execute

  # Temporal ensemble for smoother actions
  # None = disabled (use action queue)
  # 0.01 = light smoothing (exponential moving average)
  temporal_ensemble_coeff: null

  # Learning rates
  # ACT uses different LR for backbone (vision encoder) and policy head
  lr_backbone: 0.00001   # 1e-5 for vision backbone (slower)

# Training Configuration
# ----------------------
training:
  # Training steps
  # ACT typically needs fewer steps than VLA (more sample efficient)
  steps: 50000           # 50k steps (~7.6 epochs with batch_size=8)

  # Batch size (ACT can use smaller batches)
  batch_size: 8          # Per-GPU batch size (effective=40 with 5 GPUs)

  # DataLoader settings
  shuffle: true
  num_workers: 4
  pin_memory: true

  # Gradient clipping
  grad_clip_norm: 10.0   # Max gradient norm

  # Logging and saving
  log_freq: 100          # Log every N steps
  save_freq_epochs: 1.0  # Save checkpoint every N epochs

# Optimizer Configuration
# -----------------------
optimizer:
  lr: 0.00005            # Learning rate for policy (5e-5)
  betas: [0.9, 0.999]    # Adam betas
  weight_decay: 0.0001   # Weight decay (1e-4)
  eps: 0.00000001        # Adam epsilon (1e-8)

# Scheduler Configuration
# -----------------------
scheduler:
  num_warmup_steps: 5000      # Warmup steps (10% of total)
  num_decay_steps: 50000      # Decay steps (matches training steps)
  peak_lr: 0.00005            # Peak learning rate (5e-5)
  decay_lr: 0.000005          # Final learning rate (5e-6)

# Notes:
# ------
# 1. ACT is more sample-efficient than VLA, fewer episodes needed
# 2. Larger action chunks (100) → smoother, more consistent actions
# 3. Different LR for backbone vs policy head (lr_backbone < lr)
# 4. ACT excels at contact-rich, precision tasks (peg insertion)
# 5. Temporal ensemble can be enabled for smoother execution
#
# Performance expectations:
# - 50 demos → ~80% success rate (ACT paper)
# - 100 demos → ~90% success rate
# - Trains in ~3-5 hours on 5 GPUs
